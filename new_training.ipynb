{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siALwsf1eM_M",
        "outputId": "554afc27-9cd9-45e0-87c6-80f48689a8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch epochs20 https://github.com/Anjaas85/authentic_vs_AI_generated_photos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBhKJIMRecEV",
        "outputId": "e9e8d710-9fb8-44a9-becf-39d945fa28f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'authentic_vs_AI_generated_photos'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 75 (delta 4), reused 7 (delta 1), pack-reused 62 (from 1)\u001b[K\n",
            "Receiving objects: 100% (75/75), 191.34 MiB | 12.61 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd authentic_vs_AI_generated_photos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSXZQ1NoejPs",
        "outputId": "ec2b78cf-6249-4252-e578-3007d25c0c6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/authentic_vs_AI_generated_photos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zws0ilmepos",
        "outputId": "68b5f9b6-ceab-47d4-8d16-453d8f6dfe94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic_graphics.ipynb  data  metrics  training20ep.ipynb\t\t     utils\n",
            "checkpoints\t      logs  models   two_class_classification.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch new_branch"
      ],
      "metadata": {
        "id": "0farjProoGs_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout new_branch"
      ],
      "metadata": {
        "id": "9nab_HGooSW_",
        "outputId": "b03bf4e6-35f8-4d7d-8e52-f59441bb45e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'new_branch'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and Distributing the dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Define paths\n",
        "data_root = \"./data\"\n",
        "train_dir = os.path.join(data_root, \"train\")\n",
        "val_dir = os.path.join(data_root, \"val\")\n",
        "class_dirs = [\"class_0\", \"class_1\"]\n",
        "\n",
        "# Step 2: Create folder structure\n",
        "for directory in [train_dir, val_dir]:\n",
        "    for class_dir in class_dirs:\n",
        "        os.makedirs(os.path.join(directory, class_dir), exist_ok=True)\n",
        "\n",
        "# Step 3: Download CIFAR10 dataset\n",
        "cifar10 = CIFAR10(root=\"./\", download=True)\n",
        "data, labels = cifar10.data, cifar10.targets\n",
        "\n",
        "# Step 4: Simulate CIFAKE10 (first 5 classes as \"authentic\", next 5 as \"AI-generated\")\n",
        "labels = [0 if label < 5 else 1 for label in labels]\n",
        "\n",
        "# Step 5: Split data into train and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Helper function to save images\n",
        "def save_images(images, labels, save_dir):\n",
        "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
        "        class_folder = os.path.join(save_dir, f\"class_{label}\")\n",
        "        img = Image.fromarray(image)\n",
        "        img.save(os.path.join(class_folder, f\"{idx}.png\"))\n",
        "\n",
        "# Step 6: Save images to train and val folders\n",
        "save_images(train_data, train_labels, train_dir)\n",
        "save_images(val_data, val_labels, val_dir)\n",
        "\n",
        "print(\"CIFAKE10 dataset distributed into train and validation folders.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from models.densenet_model import create_densenet\n",
        "from utils.data_loader import create_dataloaders, create_test_loader\n",
        "from utils.evaluation import evaluate_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, f1_score\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Paths and parameters\n",
        "data_dir = 'data'\n",
        "\n",
        "# Load train, validation, and test data\n",
        "dataloaders = create_dataloaders(data_dir, batch_size=32)\n",
        "test_loader = create_test_loader( batch_size=32)\n",
        "\n",
        "# Load model\n",
        "model = create_densenet(num_classes=2).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay = 1e-4)\n",
        "\n",
        "#learnin rate\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 3, verbose = True)\n",
        "\n",
        "\"\"\"\n",
        "def train_model(model, criterion, optimizer, dataloaders, num_epochs=20, device='cpu'):\n",
        "    # Set up folders\n",
        "    metrics_file_path = os.path.join('metrics', 'metrics_by_epoch.txt')\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "    # Initialize storage for metrics\n",
        "    metrics_data = {\n",
        "        \"precision_recall\": [],\n",
        "        \"f1_scores\": [],\n",
        "        \"val_probs\": [],\n",
        "        \"val_labels\": [],\n",
        "        \"confusion_matrices\": [],\n",
        "        \"epoch_metrics\": []\n",
        "    }\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "    with open(metrics_file_path, 'w') as f:\n",
        "        f.write(f'Epoch\\tPhase\\tLoss\\tAccuracy\\n')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "                all_labels = []\n",
        "                all_probs = []\n",
        "\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    all_labels.extend(labels.cpu().numpy().tolist())\n",
        "                    all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
        "\n",
        "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "                f.write(f'{epoch+1}\\t{phase}\\t{epoch_loss:.4f}\\t{epoch_acc:.4f}\\n')\n",
        "\n",
        "                if phase == 'val':\n",
        "                    scheduler.step(epoch_loss)\n",
        "                    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n",
        "                    f1 = f1_score(all_labels, np.round(all_probs))\n",
        "                    confusion_mat = confusion_matrix(all_labels, np.round(all_probs))\n",
        "\n",
        "                    metrics_data[\"precision_recall\"].append({\n",
        "                        \"precision\": precision.tolist(),\n",
        "                        \"recall\": recall.tolist(),\n",
        "                        \"thresholds\": thresholds.tolist(),\n",
        "                    })\n",
        "                    metrics_data[\"f1_scores\"].append(float(f1))\n",
        "                    metrics_data[\"val_probs\"].append(all_probs)\n",
        "                    metrics_data[\"val_labels\"].append(all_labels)\n",
        "                    metrics_data[\"confusion_matrices\"].append(confusion_mat.tolist())\n",
        "                    metrics_data[\"epoch_metrics\"].append({\n",
        "                        \"epoch\": epoch+1,\n",
        "                        \"loss\": float(epoch_loss),\n",
        "                        \"accuracy\": float(epoch_acc)\n",
        "                    })\n",
        "\n",
        "    # Save metrics data as JSON\n",
        "    with open(os.path.join('metrics', 'metrics_data.json'), 'w') as json_file:\n",
        "        json.dump(metrics_data, json_file, indent=4)\n",
        "\n",
        "    # Save final model\n",
        "    model_save_path = os.path.join('checkpoints', 'densenet_ai_vs_authentic.pth')\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "\n",
        "    print(f\"Metrics saved in {metrics_file_path} and metrics_data.json\")\n",
        "    return model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import json\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "\n",
        "# Improved training loop with GPU support and metric saving\n",
        "def train_model(model, criterion, optimizer, dataloaders, num_epochs=20, device='cuda'):\n",
        "    # Set up folders for saving metrics and model\n",
        "    metrics_file_path = os.path.join('metrics', 'metrics_by_epoch.txt')\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "    # Initialize storage for metrics\n",
        "    metrics_data = {\n",
        "        \"precision_recall\": [],\n",
        "        \"f1_scores\": [],\n",
        "        \"val_probs\": [],\n",
        "        \"val_labels\": [],\n",
        "        \"confusion_matrices\": [],\n",
        "        \"epoch_metrics\": []\n",
        "    }\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "    with open(metrics_file_path, 'w') as f:\n",
        "        f.write(f'Epoch\\tPhase\\tLoss\\tAccuracy\\n')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Training and Validation phases\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "                all_labels = []\n",
        "                all_probs = []\n",
        "\n",
        "                # Iterate through batches in dataloader\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    # Move data to the correct device (GPU)\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Forward pass\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()  # Backpropagation\n",
        "                            optimizer.step()  # Update weights\n",
        "\n",
        "                    # Update running loss and correct predictions\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    all_labels.extend(labels.cpu().numpy().tolist())\n",
        "                    all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
        "\n",
        "                # Calculate and log epoch statistics\n",
        "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "                f.write(f'{epoch+1}\\t{phase}\\t{epoch_loss:.4f}\\t{epoch_acc:.4f}\\n')\n",
        "\n",
        "                if phase == 'val':\n",
        "                    # Step the scheduler with validation loss\n",
        "                    scheduler.step(epoch_loss)\n",
        "\n",
        "                    # Calculate additional metrics for evaluation\n",
        "                    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n",
        "                    f1 = f1_score(all_labels, np.round(all_probs))\n",
        "                    confusion_mat = confusion_matrix(all_labels, np.round(all_probs))\n",
        "\n",
        "                    metrics_data[\"precision_recall\"].append({\n",
        "                        \"precision\": precision.tolist(),\n",
        "                        \"recall\": recall.tolist(),\n",
        "                        \"thresholds\": thresholds.tolist(),\n",
        "                    })\n",
        "                    metrics_data[\"f1_scores\"].append(float(f1))\n",
        "                    metrics_data[\"val_probs\"].append(all_probs)\n",
        "                    metrics_data[\"val_labels\"].append(all_labels)\n",
        "                    metrics_data[\"confusion_matrices\"].append(confusion_mat.tolist())\n",
        "                    metrics_data[\"epoch_metrics\"].append({\n",
        "                        \"epoch\": epoch+1,\n",
        "                        \"loss\": float(epoch_loss),\n",
        "                        \"accuracy\": float(epoch_acc)\n",
        "                    })\n",
        "\n",
        "    # Save metrics data as JSON\n",
        "    with open(os.path.join('metrics', 'metrics_data.json'), 'w') as json_file:\n",
        "        json.dump(metrics_data, json_file, indent=4)\n",
        "\n",
        "    # Save final trained model\n",
        "    model_save_path = os.path.join('checkpoints', 'densenet_ai_vs_authentic.pth')\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "\n",
        "    print(f\"Metrics saved in {metrics_file_path} and metrics_data.json\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, criterion, optimizer, dataloaders, num_epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(trained_model.state_dict(), 'checkpoints/densenet_ai_vs_authentic.pth')\n",
        "print(\"Model trained and saved!\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating on validation set:\")\n",
        "evaluate_model(trained_model, dataloaders['val'], device,\"validation\")\n",
        "\n",
        "print(\"Evaluating on test set:\")\n",
        "evaluate_model(trained_model, test_loader, device, \"test\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WxjQNkie3nM",
        "outputId": "ac63c0f5-4c8f-4fcb-a63c-08162b936cc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "CIFAKE10 dataset distributed into train and validation folders.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.6644 Acc: 0.6032\n",
            "val Loss: 0.5931 Acc: 0.6835\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5611 Acc: 0.7080\n",
            "val Loss: 0.4953 Acc: 0.7592\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4887 Acc: 0.7593\n",
            "val Loss: 0.4306 Acc: 0.8012\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4469 Acc: 0.7897\n",
            "val Loss: 0.4178 Acc: 0.8049\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4191 Acc: 0.8067\n",
            "val Loss: 0.3691 Acc: 0.8380\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3872 Acc: 0.8240\n",
            "val Loss: 0.3331 Acc: 0.8507\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3663 Acc: 0.8376\n",
            "val Loss: 0.3267 Acc: 0.8563\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3486 Acc: 0.8458\n",
            "val Loss: 0.2972 Acc: 0.8702\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3320 Acc: 0.8540\n",
            "val Loss: 0.2762 Acc: 0.8791\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3246 Acc: 0.8578\n",
            "val Loss: 0.2841 Acc: 0.8756\n",
            "Model saved at checkpoints/densenet_ai_vs_authentic.pth\n",
            "Metrics saved in metrics/metrics_by_epoch.txt and metrics_data.json\n",
            "Model trained and saved!\n",
            "Evaluating on validation set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88      5000\n",
            "           1       0.90      0.85      0.87      5000\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4526  474]\n",
            " [ 770 4230]]\n",
            "Evaluating on test set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88      5000\n",
            "           1       0.89      0.85      0.87      5000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4503  497]\n",
            " [ 768 4232]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"anja.stanic2001@gmail.com\"\n",
        "!git config --global user.name \"Anja Stanic\""
      ],
      "metadata": {
        "id": "PMD0MWQB2cIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"First results for initial training on test and val\""
      ],
      "metadata": {
        "id": "oPhCtrQKohR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset --soft HEAD~1\n"
      ],
      "metadata": {
        "id": "mQuEpHpQ3ylB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset"
      ],
      "metadata": {
        "id": "tiu7bEMb7qRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git log"
      ],
      "metadata": {
        "id": "G-HrvFmoonPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add new_main.ipynb"
      ],
      "metadata": {
        "id": "0Ip3Rs195Zle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add checkpoints/densenet_ai_vs_authentic.pth"
      ],
      "metadata": {
        "id": "gQcWaSMw5lMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add utils/data_loader.py\n",
        "!git add utils/evaluation.py\n",
        "!git add models/densenet_model.py"
      ],
      "metadata": {
        "id": "hafazU6B6DER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wG2Jd4t7d5F",
        "outputId": "164a801e-3e2c-4081-9de5-df7ca838f3fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch new_branch\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mdeleted:    basic_graphics.ipynb\u001b[m\n",
            "\t\u001b[31mmodified:   checkpoints/densenet_ai_vs_authentic.pth\u001b[m\n",
            "\t\u001b[31mmodified:   metrics/metrics.txt\u001b[m\n",
            "\t\u001b[31mmodified:   metrics/metrics_by_epoch.txt\u001b[m\n",
            "\t\u001b[31mmodified:   metrics/metrics_data.json\u001b[m\n",
            "\t\u001b[31mmodified:   models/densenet_model.py\u001b[m\n",
            "\t\u001b[31mdeleted:    two_class_classification.ipynb\u001b[m\n",
            "\t\u001b[31mmodified:   utils/data_loader.py\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mcifar-10-batches-py/\u001b[m\n",
            "\t\u001b[31mcifar-10-python.tar.gz\u001b[m\n",
            "\t\u001b[31mdata/train/class_0/\u001b[m\n",
            "\t\u001b[31mdata/train/class_1/\u001b[m\n",
            "\t\u001b[31mdata/val/class_0/\u001b[m\n",
            "\t\u001b[31mdata/val/class_1/\u001b[m\n",
            "\t\u001b[31mmodels/__pycache__/\u001b[m\n",
            "\t\u001b[31mutils/__pycache__/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Result of inital training for val and test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYQMP6ae6lDc",
        "outputId": "22b09805-9bad-4055-d0e4-286ea53daa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 7d673d2d] Result of inital training for val and test\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 checkpoints/densenet_ai_vs_authentic.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "DrlvvZcN7-rD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}