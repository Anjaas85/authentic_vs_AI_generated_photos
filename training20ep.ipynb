{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siALwsf1eM_M",
        "outputId": "554afc27-9cd9-45e0-87c6-80f48689a8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Anjaas85/authentic_vs_AI_generated_photos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBhKJIMRecEV",
        "outputId": "3f49f512-34e8-4e52-d98b-33353f144c3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'authentic_vs_AI_generated_photos'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 48 (delta 4), reused 11 (delta 0), pack-reused 30 (from 1)\u001b[K\n",
            "Receiving objects: 100% (48/48), 161.00 MiB | 12.61 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd authentic_vs_AI_generated_photos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSXZQ1NoejPs",
        "outputId": "e7603c4c-fdd6-460a-cb22-8e6e83711f5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/authentic_vs_AI_generated_photos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zws0ilmepos",
        "outputId": "a42917d8-04b9-4f77-f699-5f15bc15996d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints  data  logs  models  two_class_classification.ipynb  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch epochs20"
      ],
      "metadata": {
        "id": "0farjProoGs_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout epochs20"
      ],
      "metadata": {
        "id": "9nab_HGooSW_",
        "outputId": "70b5961e-2fd7-42ca-bf21-a75df33c115c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Switched to branch 'epochs20'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and Distributing the dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Define paths\n",
        "data_root = \"./data\"\n",
        "train_dir = os.path.join(data_root, \"train\")\n",
        "val_dir = os.path.join(data_root, \"val\")\n",
        "class_dirs = [\"class_0\", \"class_1\"]\n",
        "\n",
        "# Step 2: Create folder structure\n",
        "for directory in [train_dir, val_dir]:\n",
        "    for class_dir in class_dirs:\n",
        "        os.makedirs(os.path.join(directory, class_dir), exist_ok=True)\n",
        "\n",
        "# Step 3: Download CIFAR10 dataset\n",
        "cifar10 = CIFAR10(root=\"./\", download=True)\n",
        "data, labels = cifar10.data, cifar10.targets\n",
        "\n",
        "# Step 4: Simulate CIFAKE10 (first 5 classes as \"authentic\", next 5 as \"AI-generated\")\n",
        "labels = [0 if label < 5 else 1 for label in labels]\n",
        "\n",
        "# Step 5: Split data into train and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Helper function to save images\n",
        "def save_images(images, labels, save_dir):\n",
        "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
        "        class_folder = os.path.join(save_dir, f\"class_{label}\")\n",
        "        img = Image.fromarray(image)\n",
        "        img.save(os.path.join(class_folder, f\"{idx}.png\"))\n",
        "\n",
        "# Step 6: Save images to train and val folders\n",
        "save_images(train_data, train_labels, train_dir)\n",
        "save_images(val_data, val_labels, val_dir)\n",
        "\n",
        "print(\"CIFAKE10 dataset distributed into train and validation folders.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from models.densenet_model import create_densenet\n",
        "from utils.data_loader import create_dataloaders, create_test_loader\n",
        "from utils.evaluation import evaluate_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, f1_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Paths and parameters\n",
        "data_dir = 'data'\n",
        "\n",
        "# Load train, validation, and test data\n",
        "dataloaders = create_dataloaders(data_dir, batch_size=32)\n",
        "test_loader = create_test_loader( batch_size=32)\n",
        "\n",
        "# Load model\n",
        "model = create_densenet(num_classes=2).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "# Training function with extended metrics saving\n",
        "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
        "    metrics_file_path = os.path.join('metrics', 'metrics_by_epoch.txt')\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "\n",
        "    # Initialize storage for metrics\n",
        "    metrics_data = {\n",
        "        \"precision_recall\": [],\n",
        "        \"f1_scores\": [],\n",
        "        \"val_probs\": [],\n",
        "        \"val_labels\": [],\n",
        "        \"confusion_matrices\": [],\n",
        "        \"epoch_metrics\": []\n",
        "    }\n",
        "\n",
        "    with open(metrics_file_path, 'w') as f:\n",
        "        f.write(f'Epoch\\tPhase\\tLoss\\tAccuracy\\n')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "                all_labels = []\n",
        "                all_probs = []\n",
        "\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                    all_labels.extend(labels.cpu().numpy().tolist())\n",
        "                    all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
        "\n",
        "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "                # Print and save metrics\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "                f.write(f'{epoch+1}\\t{phase}\\t{epoch_loss:.4f}\\t{epoch_acc:.4f}\\n')\n",
        "\n",
        "                # Save metrics for validation phase\n",
        "                if phase == 'val':\n",
        "                    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n",
        "                    f1 = f1_score(all_labels, np.round(all_probs))\n",
        "                    confusion_mat = confusion_matrix(all_labels, np.round(all_probs))\n",
        "\n",
        "                    metrics_data[\"precision_recall\"].append({\n",
        "                        \"precision\": precision.tolist(),\n",
        "                        \"recall\": recall.tolist(),\n",
        "                        \"thresholds\": thresholds.tolist(),\n",
        "                    })\n",
        "                    metrics_data[\"f1_scores\"].append(float(f1))\n",
        "                    metrics_data[\"val_probs\"].append(all_probs)\n",
        "                    metrics_data[\"val_labels\"].append(all_labels)\n",
        "                    metrics_data[\"confusion_matrices\"].append(confusion_mat.tolist())\n",
        "\n",
        "                    metrics_data[\"epoch_metrics\"].append({\n",
        "                        \"epoch\": epoch+1,\n",
        "                        \"loss\": float(epoch_loss),\n",
        "                        \"accuracy\": float(epoch_acc)\n",
        "                    })\n",
        "\n",
        "    # Save metrics data as JSON\n",
        "    with open(os.path.join('metrics', 'metrics_data.json'), 'w') as json_file:\n",
        "        json.dump(metrics_data, json_file, indent=4)\n",
        "\n",
        "    print(f\"Metrics saved in {metrics_file_path} and metrics_data.json\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, criterion, optimizer, dataloaders, num_epochs=20)\n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(trained_model.state_dict(), 'checkpoints/densenet_ai_vs_authentic.pth')\n",
        "print(\"Model trained and saved!\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating on validation set:\")\n",
        "evaluate_model(trained_model, dataloaders['val'], device,\"validation\")\n",
        "\n",
        "print(\"Evaluating on test set:\")\n",
        "evaluate_model(trained_model, test_loader, device, \"test\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WxjQNkie3nM",
        "outputId": "561a9aae-8c5d-49df-9a2c-be4f2cfdc2ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 13.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "CIFAKE10 dataset distributed into train and validation folders.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----------\n",
            "train Loss: 0.6551 Acc: 0.6136\n",
            "val Loss: 0.6387 Acc: 0.6489\n",
            "Epoch 2/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.6002 Acc: 0.6901\n",
            "val Loss: 0.5825 Acc: 0.7093\n",
            "Epoch 3/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5647 Acc: 0.7306\n",
            "val Loss: 0.5520 Acc: 0.7444\n",
            "Epoch 4/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5379 Acc: 0.7586\n",
            "val Loss: 0.5328 Acc: 0.7673\n",
            "Epoch 5/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5225 Acc: 0.7751\n",
            "val Loss: 0.5060 Acc: 0.7924\n",
            "Epoch 6/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5086 Acc: 0.7915\n",
            "val Loss: 0.5075 Acc: 0.7864\n",
            "Epoch 7/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4985 Acc: 0.8034\n",
            "val Loss: 0.4822 Acc: 0.8183\n",
            "Epoch 8/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4872 Acc: 0.8143\n",
            "val Loss: 0.5047 Acc: 0.7975\n",
            "Epoch 9/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4808 Acc: 0.8210\n",
            "val Loss: 0.4810 Acc: 0.8194\n",
            "Epoch 10/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4766 Acc: 0.8279\n",
            "val Loss: 0.4694 Acc: 0.8348\n",
            "Epoch 11/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4664 Acc: 0.8392\n",
            "val Loss: 0.4640 Acc: 0.8386\n",
            "Epoch 12/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4610 Acc: 0.8430\n",
            "val Loss: 0.4627 Acc: 0.8407\n",
            "Epoch 13/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4547 Acc: 0.8498\n",
            "val Loss: 0.4579 Acc: 0.8472\n",
            "Epoch 14/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4535 Acc: 0.8525\n",
            "val Loss: 0.4569 Acc: 0.8458\n",
            "Epoch 15/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4469 Acc: 0.8592\n",
            "val Loss: 0.4446 Acc: 0.8611\n",
            "Epoch 16/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4413 Acc: 0.8655\n",
            "val Loss: 0.4429 Acc: 0.8633\n",
            "Epoch 17/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4424 Acc: 0.8657\n",
            "val Loss: 0.4455 Acc: 0.8598\n",
            "Epoch 18/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4360 Acc: 0.8711\n",
            "val Loss: 0.4443 Acc: 0.8604\n",
            "Epoch 19/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4341 Acc: 0.8727\n",
            "val Loss: 0.4371 Acc: 0.8673\n",
            "Epoch 20/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4298 Acc: 0.8764\n",
            "val Loss: 0.4388 Acc: 0.8688\n",
            "Metrics saved in metrics/metrics_by_epoch.txt and metrics_data.json\n",
            "Model trained and saved!\n",
            "Evaluating on validation set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87      5000\n",
            "           1       0.87      0.86      0.87      5000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4378  622]\n",
            " [ 690 4310]]\n",
            "Evaluating on test set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87      5000\n",
            "           1       0.87      0.86      0.87      5000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4362  638]\n",
            " [ 677 4323]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"anja.stanic2001@gmail.com\"\n",
        "!git config --global user.name \"Anja Stanic\""
      ],
      "metadata": {
        "id": "PMD0MWQB2cIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"First results for initial training on test and val\""
      ],
      "metadata": {
        "id": "oPhCtrQKohR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset --soft HEAD~1\n"
      ],
      "metadata": {
        "id": "mQuEpHpQ3ylB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset"
      ],
      "metadata": {
        "id": "tiu7bEMb7qRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git log"
      ],
      "metadata": {
        "id": "G-HrvFmoonPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add new_main.ipynb"
      ],
      "metadata": {
        "id": "0Ip3Rs195Zle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add checkpoints/densenet_ai_vs_authentic.pth"
      ],
      "metadata": {
        "id": "gQcWaSMw5lMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add utils/data_loader.py\n",
        "!git add utils/evaluation.py\n",
        "!git add models/densenet_model.py"
      ],
      "metadata": {
        "id": "hafazU6B6DER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wG2Jd4t7d5F",
        "outputId": "d96b2a60-3262-4ea8-842b-93334341bd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "\t\u001b[32mnew file:   checkpoints/densenet_ai_vs_authentic.pth\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m.gitignore\u001b[m\n",
            "\t\u001b[31mcifar-10-batches-py/\u001b[m\n",
            "\t\u001b[31mcifar-10-python.tar.gz\u001b[m\n",
            "\t\u001b[31mdata/train/class_0/\u001b[m\n",
            "\t\u001b[31mdata/train/class_1/\u001b[m\n",
            "\t\u001b[31mdata/val/class_0/\u001b[m\n",
            "\t\u001b[31mdata/val/class_1/\u001b[m\n",
            "\t\u001b[31mmodels/__pycache__/\u001b[m\n",
            "\t\u001b[31mutils/__pycache__/\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Result of inital training for val and test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYQMP6ae6lDc",
        "outputId": "22b09805-9bad-4055-d0e4-286ea53daa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 7d673d2d] Result of inital training for val and test\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 checkpoints/densenet_ai_vs_authentic.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "DrlvvZcN7-rD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}